# Tutorial Prompt Generator Configuration

per_execution_timeout: 86400

# Data Perception
max_file_group_size_to_show: 5
num_example_files_to_show: 1

max_chars_per_file: 768
num_tutorial_retrievals: 30
max_num_tutorials: 5
max_user_input_length: 2048
max_error_message_length: 2048
max_tutorial_length: 32768
create_venv: False
condense_tutorials: True
use_tutorial_summary: True
continuous_improvement: False
optimize_system_resources: False
cleanup_unused_env: True

# Default LLM Configuration
# For each agent (coder, etc.) you can use a different one
llm: &default_llm
  # Note: bedrock is only supported in limited AWS regions
  #       and requires AWS credentials
  provider: bedrock
  model: "apac.anthropic.claude-sonnet-4-20250514-v1:0"
  max_tokens: 65535
  proxy_url: null
  temperature: 0.1
  top_p: 0.9
  verbose: True
  multi_turn: False
  template: null
  add_coding_format_instruction: false

coder:
  <<: *default_llm
  multi_turn: True

executer:
  <<: *default_llm
  max_stdout_length: 8192
  max_stderr_length: 2048


reader:
  provider: openai
  model: "gpt-4.1-2025-04-14"
  max_tokens: 32768
  proxy_url: null
  temperature: 0.1
  top_p: 0.9
  verbose: True
  multi_turn: False
  template: null
  add_coding_format_instruction: False
  details: False

error_analyzer:
  <<: *default_llm

retriever:
  <<: *default_llm

reranker:
  <<: *default_llm
  temperature: 0.0
  top_p: 1.0

description_file_retriever:
  <<: *default_llm
  temperature: 0.0
  top_p: 1.0

task_descriptor:
  <<: *default_llm
  max_description_files_length_to_show: 1024
  max_description_files_length_for_summarization: 16384

tool_selector:
  <<: *default_llm
  temperature: 0.0
  top_p: 1.0

